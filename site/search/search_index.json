{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ML Repository Warning If you find any errors, raise an issue at here. About ML Repository is a project that aims to bridge the gap between Theoretical Machine Learning and Applied Machine Learning. Want to contribute, check out the official github repo!","title":"ML Repository"},{"location":"#ml-repository","text":"Warning If you find any errors, raise an issue at here.","title":"ML Repository"},{"location":"#about","text":"ML Repository is a project that aims to bridge the gap between Theoretical Machine Learning and Applied Machine Learning.","title":"About"},{"location":"#want-to-contribute-check-out-the-official-github-repo","text":"","title":"Want to contribute, check out the official github repo!"},{"location":"gradient_descent/","text":"Gradient Descent Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the :ref: parameters <glossary_parameters> of our model. Parameters refer to coefficients in :doc: linear_regression and :ref: weights <nn_weights> in neural networks. Introduction Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point--the direction that decreases the cost function as quickly as possible. Source Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill--a local minimum. image source . Learning rate The size of these steps is called the learning rate . With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom. Cost function A :ref: cost_function tells us \"how good\" our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate. Step-by-step Now let's run gradient descent using our new cost function. There are two parameters in our cost function we can control: \\(m\\) (weight) and \\(b\\) (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient. Math Given the cost function: $$ f(m,b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (mx_i + b))^2 $$ The gradient can be calculated as: \\[ f'(m,b) = \\begin{bmatrix} \\frac{df}{dm}\\\\ \\frac{df}{db}\\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\ \\end{bmatrix} \\] To solve for the gradient, we iterate through our data points using our new \\(m\\) and \\(b\\) values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate. Code update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def update_weights ( m , b , X , Y , learning_rate ): m_deriv = 0 b_deriv = 0 N = len ( X ) for i in range ( N ): # Calculate partial derivatives # -2x(y - (mx + b)) m_deriv += - 2 * X [ i ] * ( Y [ i ] - ( m * X [ i ] + b )) # -2(y - (mx + b)) b_deriv += - 2 * ( Y [ i ] - ( m * X [ i ] + b )) # We subtract because the derivatives point in direction of steepest ascent m -= ( m_deriv / float ( N )) * learning_rate b -= ( b_deriv / float ( N )) * learning_rate return m , b References http://ruder.io/optimizing-gradient-descent \u21a9","title":"Gradient Descent"},{"location":"gradient_descent/#gradient-descent","text":"Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the :ref: parameters <glossary_parameters> of our model. Parameters refer to coefficients in :doc: linear_regression and :ref: weights <nn_weights> in neural networks.","title":"Gradient Descent"},{"location":"gradient_descent/#introduction","text":"Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point--the direction that decreases the cost function as quickly as possible. Source Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill--a local minimum. image source .","title":"Introduction"},{"location":"gradient_descent/#learning-rate","text":"The size of these steps is called the learning rate . With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.","title":"Learning rate"},{"location":"gradient_descent/#cost-function","text":"A :ref: cost_function tells us \"how good\" our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.","title":"Cost function"},{"location":"gradient_descent/#step-by-step","text":"Now let's run gradient descent using our new cost function. There are two parameters in our cost function we can control: \\(m\\) (weight) and \\(b\\) (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.","title":"Step-by-step"},{"location":"gradient_descent/#math","text":"Given the cost function: $$ f(m,b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (mx_i + b))^2 $$ The gradient can be calculated as: \\[ f'(m,b) = \\begin{bmatrix} \\frac{df}{dm}\\\\ \\frac{df}{db}\\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\ \\end{bmatrix} \\] To solve for the gradient, we iterate through our data points using our new \\(m\\) and \\(b\\) values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.","title":"Math"},{"location":"gradient_descent/#code","text":"update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def update_weights ( m , b , X , Y , learning_rate ): m_deriv = 0 b_deriv = 0 N = len ( X ) for i in range ( N ): # Calculate partial derivatives # -2x(y - (mx + b)) m_deriv += - 2 * X [ i ] * ( Y [ i ] - ( m * X [ i ] + b )) # -2(y - (mx + b)) b_deriv += - 2 * ( Y [ i ] - ( m * X [ i ] + b )) # We subtract because the derivatives point in direction of steepest ascent m -= ( m_deriv / float ( N )) * learning_rate b -= ( b_deriv / float ( N )) * learning_rate return m , b","title":"Code"},{"location":"gradient_descent/#references","text":"http://ruder.io/optimizing-gradient-descent \u21a9","title":"References"},{"location":"linear_regression/","text":"Linear Regression Introduction Regression is the process of finding the relationship between the inputs and the outputs. It is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It's used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types: Simple regression Simple linear regression uses traditional slope-intercept form, where \\(m\\) and \\(b\\) are the variables our algorithm will try to \"learn\" to produce the most accurate predictions. \\(x\\) represents our input data and \\(y\\) represents our prediction. \\[ y = mx + b \\] Multivariable regression A more complex, multi-variable linear equation might look like this, where \\(w\\) represents the coefficients, or weights, our model will try to learn. \\[ f(x,y,z) = w_1 x + w_2 y + w_3 z \\] The variables \\(x, y, z\\) represent the attributes, or distinct pieces of information, we have about each observation. For sales predictions, these attributes might include a company's advertising spend on radio, TV, and newspapers. \\[ Sales = w_1 Radio + w_2 TV + w_3 News \\] Key ingredients of regression Learning : Formulate the regression problem as an optimization problem and then solve it by finding the best parameters. Inference : Use the estimated parameters and models to predict the unseen data points. A regression problem involves several steps. They are summarized in the below image: Simple regression Let\u2019s say we are given a dataset with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. The rows (observations) represent companies. Company Radio ($) Sales Amazon 37.8 22.1 Google 39.3 10.4 Facebook 45.9 18.3 Apple 41.3 18.5 Making predictions Our prediction function outputs an estimate of sales given a company's radio advertising spend and our current values for Weight and Bias . \\[ Sales = Weight \\cdot Radio + Bias \\] Terms Weight : the coefficient for the Radio independent variable. In machine learning we call coefficients weights . Radio : the independent variable. In machine learning we call these variables features . Bias : the intercept where our line intercepts the y-axis. In machine learning we can call intercepts bias . Bias offsets all predictions that we make. Our algorithm will try to learn the correct values for Weight and Bias. By the end of our training, our equation will approximate the line of best fit . Code predict.py 1 2 def predict_sales ( radio , weight , bias ): return weight * radio + bias Cost function The prediction function is nice, but for our purposes we don't really need it. What we need is a :doc: cost function <loss_functions> so we can start optimizing our weights. Choosing the cost-function is problem specific and that's where probability also comes into picture as without any knowledge about the distributions of \\(x_{n}\\) and \\(y_{n}\\) , there is no way to choose the best training loss function. Let's use :ref: mse as our cost function. MSE measures the average squared difference between an observation's actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights. Our goal is to minimize MSE to improve the accuracy of our model. Math Given our simple linear equation \\(y = mx + b\\) , we can calculate MSE as: \\[ MSE = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (m x_i + b))^2 \\] Note \\(N\\) is the total number of observations (data points) \\(\\frac{1}{N} \\sum_{i=1}^{n}\\) is the mean \\(y_i\\) is the actual value of an observation and \\(m x_i + b\\) is our prediction Code cost_function.py 1 2 3 4 5 6 def cost_function ( radio , sales , weight , bias ): companies = len ( radio ) total_error = 0.0 for i in range ( companies ): total_error += ( sales [ i ] - ( weight * radio [ i ] + bias )) ** 2 return total_error / companies Gradient descent To minimize MSE we use :doc: gradient_descent to calculate the gradient of our cost function. Gradient descent consists of looking at the error that our weight currently gives us, using the derivative of the cost function to find the gradient (The slope of the cost function using our current weight), and then changing our weight to move in the direction opposite of the gradient. We need to move in the opposite direction of the gradient since the gradient points up the slope instead of down it, so we move in the opposite direction to try to decrease our error. Math There are two :ref: parameters <glossary_parameters> (coefficients) in our cost function we can control: weight \\(m\\) and bias \\(b\\) . Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the :ref: chain_rule . We need the chain rule because \\((y - (mx + b))^2\\) is really 2 nested functions: the inner function \\(y - (mx + b)\\) and the outer function \\(x^2\\) . Returning to our cost function: \\[ f(m,b) = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2 \\] Using the following: \\[ (y_i - (mx_i + b))^2 = A(B(m,b)) \\] We can split the derivative into \\[ A(x) = x^2 \\frac{df}{dx} = A'(x) = 2x \\] and \\[ B(m,b) = y_i - (mx_i + b) = y_i - mx_i - b \\frac{dx}{dm} = B'(m) = 0 - x_i - 0 = -x_i \\frac{dx}{db} = B'(b) = 0 - 0 - 1 = -1 \\] And then using the :ref: chain_rule which states: \\[ \\frac{df}{dm} = \\frac{df}{dx} \\frac{dx}{dm} \\frac{df}{db} = \\frac{df}{dx} \\frac{dx}{db} \\] We then plug in each of the parts to get the following derivatives \\[ \\frac{df}{dm} = A'(B(m,f)) B'(m) = 2(y_i - (mx_i + b)) \\cdot -x_i \\frac{df}{db} = A'(B(m,f)) B'(b) = 2(y_i - (mx_i + b)) \\cdot -1 \\] We can calculate the gradient of this cost function as: \\[ \\begin{align} f'(m,b) = \\begin{bmatrix} \\frac{df}{dm}\\\\ \\frac{df}{db}\\\\ \\end{bmatrix} &= \\begin{bmatrix} \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\\\ \\end{bmatrix}\\\\ &= \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\ \\end{bmatrix} \\end{align} \\] Code To solve for the gradient, we iterate through our data points using our new weight and bias values and take the average of the partial derivatives. The resulting gradient tells us the slope of our cost function at our current position (i.e. weight and bias) and the direction we should update to reduce our cost function (we move in the direction opposite the gradient). The size of our update is controlled by the :ref: learning rate <glossary_learning_rate> . update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def update_weights ( radio , sales , weight , bias , learning_rate ): weight_deriv = 0 bias_deriv = 0 companies = len ( radio ) for i in range ( companies ): # Calculate partial derivatives # -2x(y - (mx + b)) weight_deriv += - 2 * radio [ i ] * ( sales [ i ] - ( weight * radio [ i ] + bias )) # -2(y - (mx + b)) bias_deriv += - 2 * ( sales [ i ] - ( weight * radio [ i ] + bias )) # We subtract because the derivatives point in direction of steepest ascent weight -= ( weight_deriv / companies ) * learning_rate bias -= ( bias_deriv / companies ) * learning_rate return weight , bias Training So, Find a line \\(g(x) = ax+b\\) that best fits the training data. The optimally criterion is to minimize the squared error: \\[ \\frac{1}{N} \\sum_{i=1}^{n} (y_i - g(x))^2 \\] Training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope of the cost function (gradient). Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost. Before training we need to initialize our weights (set default values), set our :ref: hyperparameters <glossary_hyperparameters> (learning rate and number of iterations), and prepare to log our progress over each iteration. Code train.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def train ( radio , sales , weight , bias , learning_rate , iters ): cost_history = [] for i in range ( iters ): weight , bias = update_weights ( radio , sales , weight , bias , learning_rate ) #Calculate cost for auditing purposes cost = cost_function ( radio , sales , weight , bias ) cost_history . append ( cost ) # Log Progress if i % 10 == 0 : print \"iter= {:d} weight= {:.2f} bias= {:.4f} cost= {:.2} \" . format ( i , weight , bias , cost ) return weight , bias , cost_history Model evaluation If our model is working, we should see our cost decrease after every iteration. Logging iter=1 weight=.03 bias=.0014 cost=197.25 iter=10 weight=.28 bias=.0116 cost=74.65 iter=20 weight=.39 bias=.0177 cost=49.48 iter=30 weight=.44 bias=.0219 cost=44.31 iter=30 weight=.46 bias=.0249 cost=43.28 Visualizing Cost history Summary By learning the best values for weight (.46) and bias (.25), we now have an equation that predicts future sales based on radio advertising investment. \\[ Sales = .46\\cdot Radio + .025 \\] How would our model perform in the real world? I\u2019ll let you think about it :) Multivariable regression Let\u2019s say we are given data on TV, radio, and newspaper advertising spend for a list of companies, and our goal is to predict sales in terms of units sold. Company TV Radio News Units Amazon 230.1 37.8 69.1 22.1 Google 44.5 39.3 23.1 10.4 Facebook 17.2 45.9 34.7 18.3 Apple 151.5 41.3 13.2 18.5 Growing complexity As the number of features grows, the complexity of our model increases and it becomes increasingly difficult to visualize, or even comprehend, our data. One solution is to break the data apart and compare 1-2 features at a time. In this example we explore how Radio and TV investment impacts Sales. Normalization As the number of features grows, calculating gradient takes longer to compute. We can speed this up by \"normalizing\" our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1. Definition Normalization is the process of translating data into the range \\([0, 1]\\) (or any other range). \\[ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \\] Here, \\(X_{max}\\) and \\(X_{min}\\) are the maximum and minimum values of the feature respectively. Standardization Don't confuse Normalization with Standardization . Standardization is another scaling technique like normalization but the values are centered around mean with a unit standard deviation i.e. we just make the values' mean equal zero and it's standard deviation equal one. \\[ X' = \\frac{X - \u03bc}{\u03c3} \\] Code For each feature column { #1 Subtract the mean of the column (mean normalization) #2 Divide by the range of the column (feature scaling) } Our input is a 200 x 3 matrix containing TV, Radio, and Newspaper data. Our output is a normalized matrix of the same shape with all values between -1 and 1. normalize.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def normalize ( features ): ''' features - (200, 3) features.T - (3, 200) We transpose the input matrix, swapping cols and rows to make vector math easier ''' for feature in features . T : fmean = np . mean ( feature ) frange = np . amax ( feature ) - np . amin ( feature ) #Vector Subtraction feature -= fmean #Vector Division feature /= frange return features Note Matrix math . Before we continue, it's important to understand basic :doc: linear_algebra concepts as well as numpy functions like numpy.dot() <https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html> _. Making predictions Our predict function outputs an estimate of sales given our current weights (coefficients) and a company's TV, radio, and newspaper spend. Our model will try to identify weight values that most reduce our cost function. \\[ Sales = W_1\\cdot TV + W_2\\cdot Radio + W_3\\cdot Newspaper \\] predict.py 1 2 3 4 5 6 7 8 9 def predict ( features , weights ): ''' features - (200, 3) weights - (3, 1) predictions - (200,1) ''' predictions = np . dot ( features , weights ) return predictions Initialize weights W1 = 0.0 W2 = 0.0 W3 = 0.0 weights = np . array ([ [ W1 ], [ W2 ], [ W3 ] ]) Cost function Now we need a cost function to audit how our model is performing. The math is the same, except we swap the \\(mx + b\\) expression for \\(W_1 x_1 + W_2 x_2 + W_3 x_3\\) . We also divide the expression by 2 to make derivative calculations simpler. \\[ MSE = \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3))^2 \\] cost_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def cost_function ( features , targets , weights ): ''' features:(200,3) targets: (200,1) weights:(3,1) returns average squared error among predictions ''' N = len ( targets ) predictions = predict ( features , weights ) # Matrix math lets use do this without looping sq_error = ( predictions - targets ) ** 2 # Return average squared error among predictions return 1.0 / ( 2 * N ) * sq_error . sum () Gradient descent Again using the :ref: chain_rule we can compute the gradient--a vector of partial derivatives describing the slope of the cost function for each weight. \\[ \\begin{align} f'(W_1) = -x_1(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\ f'(W_2) = -x_2(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\ f'(W_3) = -x_3(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\end{align} \\] update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def update_weights ( features , targets , weights , lr ): ''' Features:(200, 3) Targets: (200, 1) Weights:(3, 1) ''' predictions = predict ( features , weights ) #Extract our features x1 = features [:, 0 ] x2 = features [:, 1 ] x3 = features [:, 2 ] # Use dot product to calculate the derivative for each weight d_w1 = - x1 . dot ( targets - predictions ) d_w2 = - x2 . dot ( targets - predictions ) d_w2 = - x2 . dot ( targets - predictions ) # Multiply the mean derivative by the learning rate # and subtract from our weights (remember gradient points in direction of steepest ASCENT) weights [ 0 ][ 0 ] -= ( lr * np . mean ( d_w1 )) weights [ 1 ][ 0 ] -= ( lr * np . mean ( d_w2 )) weights [ 2 ][ 0 ] -= ( lr * np . mean ( d_w3 )) return weights And that's it! Multivariate linear regression. Simplifying with matrices The gradient descent code above has a lot of duplication. Can we improve it somehow? One way to refactor would be to loop through our features and weights--allowing our function to handle any number of features. However there is another even better technique: vectorized gradient descent . Math We use the same formula as above, but instead of operating on a single feature at a time, we use matrix multiplication to operative on all features and weights simultaneously. We replace the \\(x_i\\) terms with a single feature matrix \\(X\\) . \\[ gradient = -X(targets - predictions) \\] Code X = [ [ x1 , x2 , x3 ] [ x1 , x2 , x3 ] . . . [ x1 , x2 , x3 ] ] targets = [ [ 1 ], [ 2 ], [ 3 ] ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def update_weights_vectorized ( X , targets , weights , lr ): ''' gradient = X.T * (predictions - targets) / N X: (200, 3) Targets: (200, 1) Weights: (3, 1) ''' companies = len ( X ) #1 - Get Predictions predictions = predict ( X , weights ) #2 - Calculate error/loss error = targets - predictions #3 Transpose features from (200, 3) to (3, 200) # So we can multiply w the (200,1) error matrix. # Returns a (3,1) matrix holding 3 partial derivatives -- # one for each feature -- representing the aggregate # slope of the cost function across all observations gradient = np . dot ( - X . T , error ) #4 Take the average error derivative for each feature gradient /= companies #5 - Multiply the gradient by our learning rate gradient *= lr #6 - Subtract from our weights to minimize cost weights -= gradient return weights Bias term Our train function is the same as for simple linear regression, however we're going to make one final tweak before running: add a :ref: bias term <glossary_bias_term> to our feature matrix. In our example, it's very unlikely that sales would be zero if companies stopped advertising. Possible reasons for this might include past advertising, existing customer relationships, retail locations, and salespeople. A bias term will help us capture this base case. Code Below we add a constant 1 to our features matrix. By setting this value to 1, it turns our bias term into a constant. 1 2 bias = np . ones ( shape = ( len ( features ), 1 )) features = np . append ( bias , features , axis = 1 ) Model evaluation After training our model through 1000 iterations with a learning rate of .0005, we finally arrive at a set of weights we can use to make predictions: \\[ Sales = 4.7\\cdot TV + 3.5\\cdot Radio + .81\\cdot Newspaper + 13.9 \\] Our MSE cost dropped from 110.86 to 6.25. References https://en.wikipedia.org/wiki/Linear_regression \u21a9 http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html \u21a9 http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning \u21a9 http://people.duke.edu/~rnau/regintro.htm \u21a9 https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression \u21a9 https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms \u21a9","title":"Linear Regression"},{"location":"linear_regression/#linear-regression","text":"","title":"Linear Regression"},{"location":"linear_regression/#introduction","text":"Regression is the process of finding the relationship between the inputs and the outputs. It is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It's used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:","title":"Introduction"},{"location":"linear_regression/#simple-regression","text":"Simple linear regression uses traditional slope-intercept form, where \\(m\\) and \\(b\\) are the variables our algorithm will try to \"learn\" to produce the most accurate predictions. \\(x\\) represents our input data and \\(y\\) represents our prediction. \\[ y = mx + b \\]","title":"Simple regression"},{"location":"linear_regression/#multivariable-regression","text":"A more complex, multi-variable linear equation might look like this, where \\(w\\) represents the coefficients, or weights, our model will try to learn. \\[ f(x,y,z) = w_1 x + w_2 y + w_3 z \\] The variables \\(x, y, z\\) represent the attributes, or distinct pieces of information, we have about each observation. For sales predictions, these attributes might include a company's advertising spend on radio, TV, and newspapers. \\[ Sales = w_1 Radio + w_2 TV + w_3 News \\]","title":"Multivariable regression"},{"location":"linear_regression/#key-ingredients-of-regression","text":"Learning : Formulate the regression problem as an optimization problem and then solve it by finding the best parameters. Inference : Use the estimated parameters and models to predict the unseen data points. A regression problem involves several steps. They are summarized in the below image:","title":"Key ingredients of regression"},{"location":"linear_regression/#simple-regression_1","text":"Let\u2019s say we are given a dataset with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. The rows (observations) represent companies. Company Radio ($) Sales Amazon 37.8 22.1 Google 39.3 10.4 Facebook 45.9 18.3 Apple 41.3 18.5","title":"Simple regression"},{"location":"linear_regression/#making-predictions","text":"Our prediction function outputs an estimate of sales given a company's radio advertising spend and our current values for Weight and Bias . \\[ Sales = Weight \\cdot Radio + Bias \\] Terms Weight : the coefficient for the Radio independent variable. In machine learning we call coefficients weights . Radio : the independent variable. In machine learning we call these variables features . Bias : the intercept where our line intercepts the y-axis. In machine learning we can call intercepts bias . Bias offsets all predictions that we make. Our algorithm will try to learn the correct values for Weight and Bias. By the end of our training, our equation will approximate the line of best fit .","title":"Making predictions"},{"location":"linear_regression/#code","text":"predict.py 1 2 def predict_sales ( radio , weight , bias ): return weight * radio + bias","title":"Code"},{"location":"linear_regression/#cost-function","text":"The prediction function is nice, but for our purposes we don't really need it. What we need is a :doc: cost function <loss_functions> so we can start optimizing our weights. Choosing the cost-function is problem specific and that's where probability also comes into picture as without any knowledge about the distributions of \\(x_{n}\\) and \\(y_{n}\\) , there is no way to choose the best training loss function. Let's use :ref: mse as our cost function. MSE measures the average squared difference between an observation's actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights. Our goal is to minimize MSE to improve the accuracy of our model.","title":"Cost function"},{"location":"linear_regression/#math","text":"Given our simple linear equation \\(y = mx + b\\) , we can calculate MSE as: \\[ MSE = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (m x_i + b))^2 \\] Note \\(N\\) is the total number of observations (data points) \\(\\frac{1}{N} \\sum_{i=1}^{n}\\) is the mean \\(y_i\\) is the actual value of an observation and \\(m x_i + b\\) is our prediction","title":"Math"},{"location":"linear_regression/#code_1","text":"cost_function.py 1 2 3 4 5 6 def cost_function ( radio , sales , weight , bias ): companies = len ( radio ) total_error = 0.0 for i in range ( companies ): total_error += ( sales [ i ] - ( weight * radio [ i ] + bias )) ** 2 return total_error / companies","title":"Code"},{"location":"linear_regression/#gradient-descent","text":"To minimize MSE we use :doc: gradient_descent to calculate the gradient of our cost function. Gradient descent consists of looking at the error that our weight currently gives us, using the derivative of the cost function to find the gradient (The slope of the cost function using our current weight), and then changing our weight to move in the direction opposite of the gradient. We need to move in the opposite direction of the gradient since the gradient points up the slope instead of down it, so we move in the opposite direction to try to decrease our error.","title":"Gradient descent"},{"location":"linear_regression/#math_1","text":"There are two :ref: parameters <glossary_parameters> (coefficients) in our cost function we can control: weight \\(m\\) and bias \\(b\\) . Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the :ref: chain_rule . We need the chain rule because \\((y - (mx + b))^2\\) is really 2 nested functions: the inner function \\(y - (mx + b)\\) and the outer function \\(x^2\\) . Returning to our cost function: \\[ f(m,b) = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2 \\] Using the following: \\[ (y_i - (mx_i + b))^2 = A(B(m,b)) \\] We can split the derivative into \\[ A(x) = x^2 \\frac{df}{dx} = A'(x) = 2x \\] and \\[ B(m,b) = y_i - (mx_i + b) = y_i - mx_i - b \\frac{dx}{dm} = B'(m) = 0 - x_i - 0 = -x_i \\frac{dx}{db} = B'(b) = 0 - 0 - 1 = -1 \\] And then using the :ref: chain_rule which states: \\[ \\frac{df}{dm} = \\frac{df}{dx} \\frac{dx}{dm} \\frac{df}{db} = \\frac{df}{dx} \\frac{dx}{db} \\] We then plug in each of the parts to get the following derivatives \\[ \\frac{df}{dm} = A'(B(m,f)) B'(m) = 2(y_i - (mx_i + b)) \\cdot -x_i \\frac{df}{db} = A'(B(m,f)) B'(b) = 2(y_i - (mx_i + b)) \\cdot -1 \\] We can calculate the gradient of this cost function as: \\[ \\begin{align} f'(m,b) = \\begin{bmatrix} \\frac{df}{dm}\\\\ \\frac{df}{db}\\\\ \\end{bmatrix} &= \\begin{bmatrix} \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\\\ \\end{bmatrix}\\\\ &= \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\ \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\ \\end{bmatrix} \\end{align} \\]","title":"Math"},{"location":"linear_regression/#code_2","text":"To solve for the gradient, we iterate through our data points using our new weight and bias values and take the average of the partial derivatives. The resulting gradient tells us the slope of our cost function at our current position (i.e. weight and bias) and the direction we should update to reduce our cost function (we move in the direction opposite the gradient). The size of our update is controlled by the :ref: learning rate <glossary_learning_rate> . update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def update_weights ( radio , sales , weight , bias , learning_rate ): weight_deriv = 0 bias_deriv = 0 companies = len ( radio ) for i in range ( companies ): # Calculate partial derivatives # -2x(y - (mx + b)) weight_deriv += - 2 * radio [ i ] * ( sales [ i ] - ( weight * radio [ i ] + bias )) # -2(y - (mx + b)) bias_deriv += - 2 * ( sales [ i ] - ( weight * radio [ i ] + bias )) # We subtract because the derivatives point in direction of steepest ascent weight -= ( weight_deriv / companies ) * learning_rate bias -= ( bias_deriv / companies ) * learning_rate return weight , bias","title":"Code"},{"location":"linear_regression/#training","text":"So, Find a line \\(g(x) = ax+b\\) that best fits the training data. The optimally criterion is to minimize the squared error: \\[ \\frac{1}{N} \\sum_{i=1}^{n} (y_i - g(x))^2 \\] Training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope of the cost function (gradient). Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost. Before training we need to initialize our weights (set default values), set our :ref: hyperparameters <glossary_hyperparameters> (learning rate and number of iterations), and prepare to log our progress over each iteration.","title":"Training"},{"location":"linear_regression/#code_3","text":"train.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def train ( radio , sales , weight , bias , learning_rate , iters ): cost_history = [] for i in range ( iters ): weight , bias = update_weights ( radio , sales , weight , bias , learning_rate ) #Calculate cost for auditing purposes cost = cost_function ( radio , sales , weight , bias ) cost_history . append ( cost ) # Log Progress if i % 10 == 0 : print \"iter= {:d} weight= {:.2f} bias= {:.4f} cost= {:.2} \" . format ( i , weight , bias , cost ) return weight , bias , cost_history","title":"Code"},{"location":"linear_regression/#model-evaluation","text":"If our model is working, we should see our cost decrease after every iteration.","title":"Model evaluation"},{"location":"linear_regression/#logging","text":"iter=1 weight=.03 bias=.0014 cost=197.25 iter=10 weight=.28 bias=.0116 cost=74.65 iter=20 weight=.39 bias=.0177 cost=49.48 iter=30 weight=.44 bias=.0219 cost=44.31 iter=30 weight=.46 bias=.0249 cost=43.28","title":"Logging"},{"location":"linear_regression/#visualizing","text":"","title":"Visualizing"},{"location":"linear_regression/#cost-history","text":"","title":"Cost history"},{"location":"linear_regression/#summary","text":"By learning the best values for weight (.46) and bias (.25), we now have an equation that predicts future sales based on radio advertising investment. \\[ Sales = .46\\cdot Radio + .025 \\] How would our model perform in the real world? I\u2019ll let you think about it :)","title":"Summary"},{"location":"linear_regression/#multivariable-regression_1","text":"Let\u2019s say we are given data on TV, radio, and newspaper advertising spend for a list of companies, and our goal is to predict sales in terms of units sold. Company TV Radio News Units Amazon 230.1 37.8 69.1 22.1 Google 44.5 39.3 23.1 10.4 Facebook 17.2 45.9 34.7 18.3 Apple 151.5 41.3 13.2 18.5","title":"Multivariable regression"},{"location":"linear_regression/#growing-complexity","text":"As the number of features grows, the complexity of our model increases and it becomes increasingly difficult to visualize, or even comprehend, our data. One solution is to break the data apart and compare 1-2 features at a time. In this example we explore how Radio and TV investment impacts Sales.","title":"Growing complexity"},{"location":"linear_regression/#normalization","text":"As the number of features grows, calculating gradient takes longer to compute. We can speed this up by \"normalizing\" our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1. Definition Normalization is the process of translating data into the range \\([0, 1]\\) (or any other range). \\[ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \\] Here, \\(X_{max}\\) and \\(X_{min}\\) are the maximum and minimum values of the feature respectively. Standardization Don't confuse Normalization with Standardization . Standardization is another scaling technique like normalization but the values are centered around mean with a unit standard deviation i.e. we just make the values' mean equal zero and it's standard deviation equal one. \\[ X' = \\frac{X - \u03bc}{\u03c3} \\]","title":"Normalization"},{"location":"linear_regression/#code_4","text":"For each feature column { #1 Subtract the mean of the column (mean normalization) #2 Divide by the range of the column (feature scaling) } Our input is a 200 x 3 matrix containing TV, Radio, and Newspaper data. Our output is a normalized matrix of the same shape with all values between -1 and 1. normalize.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def normalize ( features ): ''' features - (200, 3) features.T - (3, 200) We transpose the input matrix, swapping cols and rows to make vector math easier ''' for feature in features . T : fmean = np . mean ( feature ) frange = np . amax ( feature ) - np . amin ( feature ) #Vector Subtraction feature -= fmean #Vector Division feature /= frange return features Note Matrix math . Before we continue, it's important to understand basic :doc: linear_algebra concepts as well as numpy functions like numpy.dot() <https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html> _.","title":"Code"},{"location":"linear_regression/#making-predictions_1","text":"Our predict function outputs an estimate of sales given our current weights (coefficients) and a company's TV, radio, and newspaper spend. Our model will try to identify weight values that most reduce our cost function. \\[ Sales = W_1\\cdot TV + W_2\\cdot Radio + W_3\\cdot Newspaper \\] predict.py 1 2 3 4 5 6 7 8 9 def predict ( features , weights ): ''' features - (200, 3) weights - (3, 1) predictions - (200,1) ''' predictions = np . dot ( features , weights ) return predictions","title":"Making predictions"},{"location":"linear_regression/#initialize-weights","text":"W1 = 0.0 W2 = 0.0 W3 = 0.0 weights = np . array ([ [ W1 ], [ W2 ], [ W3 ] ])","title":"Initialize weights"},{"location":"linear_regression/#cost-function_1","text":"Now we need a cost function to audit how our model is performing. The math is the same, except we swap the \\(mx + b\\) expression for \\(W_1 x_1 + W_2 x_2 + W_3 x_3\\) . We also divide the expression by 2 to make derivative calculations simpler. \\[ MSE = \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3))^2 \\] cost_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def cost_function ( features , targets , weights ): ''' features:(200,3) targets: (200,1) weights:(3,1) returns average squared error among predictions ''' N = len ( targets ) predictions = predict ( features , weights ) # Matrix math lets use do this without looping sq_error = ( predictions - targets ) ** 2 # Return average squared error among predictions return 1.0 / ( 2 * N ) * sq_error . sum ()","title":"Cost function"},{"location":"linear_regression/#gradient-descent_1","text":"Again using the :ref: chain_rule we can compute the gradient--a vector of partial derivatives describing the slope of the cost function for each weight. \\[ \\begin{align} f'(W_1) = -x_1(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\ f'(W_2) = -x_2(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\ f'(W_3) = -x_3(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\end{align} \\] update_weights.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def update_weights ( features , targets , weights , lr ): ''' Features:(200, 3) Targets: (200, 1) Weights:(3, 1) ''' predictions = predict ( features , weights ) #Extract our features x1 = features [:, 0 ] x2 = features [:, 1 ] x3 = features [:, 2 ] # Use dot product to calculate the derivative for each weight d_w1 = - x1 . dot ( targets - predictions ) d_w2 = - x2 . dot ( targets - predictions ) d_w2 = - x2 . dot ( targets - predictions ) # Multiply the mean derivative by the learning rate # and subtract from our weights (remember gradient points in direction of steepest ASCENT) weights [ 0 ][ 0 ] -= ( lr * np . mean ( d_w1 )) weights [ 1 ][ 0 ] -= ( lr * np . mean ( d_w2 )) weights [ 2 ][ 0 ] -= ( lr * np . mean ( d_w3 )) return weights And that's it! Multivariate linear regression.","title":"Gradient descent"},{"location":"linear_regression/#simplifying-with-matrices","text":"The gradient descent code above has a lot of duplication. Can we improve it somehow? One way to refactor would be to loop through our features and weights--allowing our function to handle any number of features. However there is another even better technique: vectorized gradient descent .","title":"Simplifying with matrices"},{"location":"linear_regression/#math_2","text":"We use the same formula as above, but instead of operating on a single feature at a time, we use matrix multiplication to operative on all features and weights simultaneously. We replace the \\(x_i\\) terms with a single feature matrix \\(X\\) . \\[ gradient = -X(targets - predictions) \\]","title":"Math"},{"location":"linear_regression/#code_5","text":"X = [ [ x1 , x2 , x3 ] [ x1 , x2 , x3 ] . . . [ x1 , x2 , x3 ] ] targets = [ [ 1 ], [ 2 ], [ 3 ] ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def update_weights_vectorized ( X , targets , weights , lr ): ''' gradient = X.T * (predictions - targets) / N X: (200, 3) Targets: (200, 1) Weights: (3, 1) ''' companies = len ( X ) #1 - Get Predictions predictions = predict ( X , weights ) #2 - Calculate error/loss error = targets - predictions #3 Transpose features from (200, 3) to (3, 200) # So we can multiply w the (200,1) error matrix. # Returns a (3,1) matrix holding 3 partial derivatives -- # one for each feature -- representing the aggregate # slope of the cost function across all observations gradient = np . dot ( - X . T , error ) #4 Take the average error derivative for each feature gradient /= companies #5 - Multiply the gradient by our learning rate gradient *= lr #6 - Subtract from our weights to minimize cost weights -= gradient return weights","title":"Code"},{"location":"linear_regression/#bias-term","text":"Our train function is the same as for simple linear regression, however we're going to make one final tweak before running: add a :ref: bias term <glossary_bias_term> to our feature matrix. In our example, it's very unlikely that sales would be zero if companies stopped advertising. Possible reasons for this might include past advertising, existing customer relationships, retail locations, and salespeople. A bias term will help us capture this base case.","title":"Bias term"},{"location":"linear_regression/#code_6","text":"Below we add a constant 1 to our features matrix. By setting this value to 1, it turns our bias term into a constant. 1 2 bias = np . ones ( shape = ( len ( features ), 1 )) features = np . append ( bias , features , axis = 1 )","title":"Code"},{"location":"linear_regression/#model-evaluation_1","text":"After training our model through 1000 iterations with a learning rate of .0005, we finally arrive at a set of weights we can use to make predictions: \\[ Sales = 4.7\\cdot TV + 3.5\\cdot Radio + .81\\cdot Newspaper + 13.9 \\] Our MSE cost dropped from 110.86 to 6.25.","title":"Model evaluation"},{"location":"linear_regression/#references","text":"https://en.wikipedia.org/wiki/Linear_regression \u21a9 http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html \u21a9 http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning \u21a9 http://people.duke.edu/~rnau/regintro.htm \u21a9 https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression \u21a9 https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms \u21a9","title":"References"},{"location":"logistic_regression/","text":"Logistic Regression Introduction Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. In other words, Logistic Regression estimates the probability that an instance belongs to a particular class. If the estimated probability is > 50%, then the model predicts in positive otherwise in negative. Comparison to linear regression Given data on time spent studying and exam scores. :doc: linear_regression and logistic regression can predict different things: Linear Regression could help us predict the student's test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range). Logistic Regression could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model's classifications. Types of logistic regression Binary (Pass/Fail) Multi (Cats, Dogs, Sheep) Ordinal (Low, Medium, High) Binary logistic regression Say we're given data on student exam results and our goal is to predict whether a student will pass or fail based on number of hours slept and hours spent studying. We have two features (hours slept, hours studied) and two classes: passed (1) and failed (0). Studied Slept Passed 4.85 9.63 1 8.62 3.23 0 5.43 8.23 1 9.21 6.34 0 Graphically we could represent our data with a scatter plot. Sigmoid activation In order to map predicted values to probabilities, we use the :ref: sigmoid <activation_sigmoid> function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities. Math \\[ S(z) = \\frac{1} {1 + e^{-z}} \\] Note \\(s(z)\\) = output between 0 and 1 (probability estimate) \\(z\\) = input to the function (your algorithm's prediction e.g. mx + b) \\(e\\) = base of natural log Graph Code sigmoid.py import numpy as np def sigmoid ( z ): return 1.0 / ( 1 + np . exp ( - z )) Decision boundary Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class (true/false, cat/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 2. \\[ p \\geq 0.5, class=1 \\] \\[ p < 0.5, class=0 \\] For example, if our threshold was 0.5 and our prediction function returned 0.7, we would classify this observation as positive. If our prediction was 0.2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability. Making predictions Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function. A prediction function in logistic regression returns the probability of our observation being positive, True, or \"Yes\". We call this class 1 and its notation is \\(P(class=1)\\) . As the probability gets closer to 1, our model is more confident that the observation is in class 1. Math Let's use the same :ref: multiple linear regression <multiple_linear_regression_predict> equation from our linear regression tutorial. \\[ z = W_0 + W_1\\cdot Studied + W_2\\cdot Slept \\] This time however we will transform the output using the sigmoid function to return a probability value between 0 and 1. \\[ P(class=1) = \\frac{1} {1 + e^{-z}} \\] If the model returns 0.4 it believes there is only a 40% chance of passing. If our decision boundary was 0.5, we would categorize this observation as \"Fail.\"\" Code We wrap the sigmoid function over the same prediction function we used in :ref: multiple linear regression <multiple_linear_regression_predict> predict.py import numpy from sigmoid import sigmoid def predict ( features , weights ): ''' Returns 1D array of probabilities that the class label == 1 ''' z = np . dot ( features , weights ) return sigmoid ( z ) Cost function Unfortunately we can't (or at least shouldn't) use the same cost function :ref: mse as we did for linear regression. Why? There is a great math explanation in chapter 3 of Michael Neilson's deep learning book 5 , but for now I'll simply say it's because our prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum. Math Instead of Mean Squared Error, we use a cost function called :ref: loss_cross_entropy , also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for \\(y=1\\) and one for \\(y=0\\) . The benefits of taking the logarithm reveal themselves when you look at the cost function graphs for y=1 and y=0. These smooth monotonic functions 7 (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost. Image from Andrew Ng's slides on logistic regression The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions! The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function. Above functions compressed into one Multiplying by \\(y\\) and \\((1-y)\\) in the above equation is a sneaky trick that let's us use the same equation to solve for both y=1 and y=0 cases. If y=0, the first side cancels out. If y=1, the second side cancels out. In both cases we only perform the operation we need to perform. Vectorized cost function Code cost_function.py def cost_function ( features , labels , weights ): ''' Using Mean Absolute Error Features:(100,3) Labels: (100,1) Weights:(3,1) Returns 1D matrix of predictions Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels) ''' observations = len ( labels ) predictions = predict ( features , weights ) #Take the error when label=1 class1_cost = - labels * np . log ( predictions ) #Take the error when label=0 class2_cost = ( 1 - labels ) * np . log ( 1 - predictions ) #Take the sum of both costs cost = class1_cost - class2_cost #Take the average cost cost = cost . sum () / observations return cost Gradient descent To minimize our cost, we use :doc: gradient_descent just like before in :doc: linear_regression . There are other more sophisticated optimization algorithms out there such as conjugate gradient like :ref: optimizers_lbfgs , but you don't have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things! Math One of the neat properties of the sigmoid function is its derivative is easy to calculate. If you're curious, there is a good walk-through derivation on stack overflow 6 . Michael Neilson also covers the topic in chapter 3 of his book. \\[ \\begin{align} s'(z) & = s(z)(1 - s(z)) \\end{align} \\] Which leads to an equally beautiful and convenient cost function derivative: \\[ C' = x(s(z) - y) \\] Note \\(C'\\) is the derivative of cost with respect to weights \\(y\\) is the actual class label (0 or 1) \\(s(z)\\) is your model's prediction \\(x\\) is your feature or feature vector. Notice how this gradient is the same as the :ref: mse gradient, the only difference is the hypothesis function. Pseudocode Repeat { 1. Calculate gradient average 2. Multiply by learning rate 3. Subtract from weights } Code update_weights.py def update_weights ( features , labels , weights , lr ): ''' Vectorized Gradient Descent Features:(200, 3) Labels: (200, 1) Weights:(3, 1) ''' N = len ( features ) #1 - Get Predictions predictions = predict ( features , weights ) #2 Transpose features from (200, 3) to (3, 200) # So we can multiply w the (200,1) cost matrix. # Returns a (3,1) matrix holding 3 partial derivatives -- # one for each feature -- representing the aggregate # slope of the cost function across all observations gradient = np . dot ( features . T , predictions - labels ) #3 Take the average cost derivative for each feature gradient /= N #4 - Multiply the gradient by our learning rate gradient *= lr #5 - Subtract from our weights to minimize cost weights -= gradient return weights Mapping probabilities to classes The final step is assign class labels (0 or 1) to our predicted probabilities. Decision boundary decision_boundary.py def decision_boundary ( prob ): return 1 if prob >= .5 else 0 Convert probabilities to classes classify.py def classify ( predictions ): ''' input - N element array of predictions between 0 and 1 output - N element array of 0s (False) and 1s (True) ''' decision_boundary = np . vectorize ( decision_boundary ) return decision_boundary ( predictions ) . flatten () Example output Probabilities = [ 0.967, 0.448, 0.015, 0.780, 0.978, 0.004] Classifications = [1, 0, 0, 1, 1, 0] Training Our training code is the same as we used for :ref: linear regression <simple_linear_regression_training> . train.py def train ( features , labels , weights , lr , iters ): cost_history = [] for i in range ( iters ): weights = update_weights ( features , labels , weights , lr ) #Calculate error for auditing purposes cost = cost_function ( features , labels , weights ) cost_history . append ( cost ) # Log Progress if i % 1000 == 0 : print \"iter: \" + str ( i ) + \" cost: \" + str ( cost ) return weights , cost_history Model evaluation If our model is working, we should see our cost decrease after every iteration. iter: 0 cost: 0.635 iter: 1000 cost: 0.302 iter: 2000 cost: 0.264 Final cost: 0.2487. Final weights: [-8.197, .921, .738] Cost history Accuracy :ref: Accuracy <glossary_accuracy> measures how correct our predictions were. In this case we simply compare predicted labels to true labels and divide by the total. accuracy.py def accuracy ( predicted_labels , actual_labels ): diff = predicted_labels - actual_labels return 1.0 - ( float ( np . count_nonzero ( diff )) / len ( diff )) Decision boundary Another helpful technique is to plot the decision boundary on top of our predictions to see how our labels compare to the actual labels. This involves plotting our predicted probabilities and coloring them with their true labels. Code to plot the decision boundary plot_decision_boundary.py def plot_decision_boundary ( trues , falses ): fig = plt . figure () ax = fig . add_subplot ( 111 ) no_of_preds = len ( trues ) + len ( falses ) ax . scatter ([ i for i in range ( len ( trues ))], trues , s = 25 , c = 'b' , marker = \"o\" , label = 'Trues' ) ax . scatter ([ i for i in range ( len ( falses ))], falses , s = 25 , c = 'r' , marker = \"s\" , label = 'Falses' ) plt . legend ( loc = 'upper right' ); ax . set_title ( \"Decision Boundary\" ) ax . set_xlabel ( 'N/2' ) ax . set_ylabel ( 'Predicted Probability' ) plt . axhline ( .5 , color = 'black' ) plt . show () Multiclass logistic regression Instead of \\(y = {0,1}\\) we will expand our definition so that \\(y = {0,1...n}\\) . Basically we re-run binary classification multiple times, once for each class. Procedure Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?). For each class... Predict the probability the observations are in that single class. prediction = \\(max(\\text{probability of the classes})\\) For each sub-problem, we select one class (YES) and lump all the others into a second class (NO). Then we take the class with the highest predicted value. Softmax activation The softmax function (softargmax or normalized exponential function) is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval [ 0 , 1 ] , and the components will add up to 1, so that they can be interpreted as probabilities. The standard (unit) softmax function is defined by the formula \\[ \\begin{align} \u03c3(z_i) = \\frac{e^{z_{(i)}}}{\\sum_{j=1}^K e^{z_{(j)}}}\\ \\ \\ for\\ i=1,.,.,.,K\\ and\\ z=z_1,.,.,.,z_K \\end{align} \\] In words: we apply the standard exponential function to each element \\(z_i\\) of the input vector \\(z\\) and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector \\(\u03c3(z)\\) is 1. 9 Scikit-Learn example Let's compare our performance to the LogisticRegression model provided by scikit-learn 8 . .browserslistrc import sklearn from sklearn.linear_model import LogisticRegression from sklearn.cross_validation import train_test_split # Normalize grades to values between 0 and 1 for more efficient computation normalized_range = sklearn . preprocessing . MinMaxScaler ( feature_range = ( - 1 , 1 )) # Extract Features + Labels labels . shape = ( 100 ,) #scikit expects this features = normalized_range . fit_transform ( features ) # Create Test/Train features_train , features_test , labels_train , labels_test = train_test_split ( features , labels , test_size = 0.4 ) # Scikit Logistic Regression scikit_log_reg = LogisticRegression () scikit_log_reg . fit ( features_train , labels_train ) #Score is Mean Accuracy scikit_score = clf . score ( features_test , labels_test ) print ( 'Scikit score: ' , scikit_score ) #Our Mean Accuracy observations , features , labels , weights = run () probabilities = predict ( features , weights ) . flatten () classifications = classifier ( probabilities ) our_acc = accuracy ( classifications , labels . flatten ()) print ( 'Our score: ' , our_acc ) Scikit score: 0.88. Our score: 0.89 References http://www.holehouse.org/mlclass/06_Logistic_Regression.html \u21a9 http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning \u21a9 https://scilab.io/machine-learning-logistic-regression-tutorial/ \u21a9 https://github.com/perborgen/LogisticRegression/blob/master/logistic.py \u21a9 http://neuralnetworksanddeeplearning.com/chap3.html \u21a9 http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x \u21a9 https://en.wikipedia.org/wiki/Monotoniconotonic_function \u21a9 http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression> \u21a9 https://en.wikipedia.org/wiki/Softmax_function \u21a9","title":"Logistic Regression"},{"location":"logistic_regression/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"logistic_regression/#introduction","text":"Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. In other words, Logistic Regression estimates the probability that an instance belongs to a particular class. If the estimated probability is > 50%, then the model predicts in positive otherwise in negative.","title":"Introduction"},{"location":"logistic_regression/#comparison-to-linear-regression","text":"Given data on time spent studying and exam scores. :doc: linear_regression and logistic regression can predict different things: Linear Regression could help us predict the student's test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range). Logistic Regression could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model's classifications.","title":"Comparison to linear regression"},{"location":"logistic_regression/#types-of-logistic-regression","text":"Binary (Pass/Fail) Multi (Cats, Dogs, Sheep) Ordinal (Low, Medium, High)","title":"Types of logistic regression"},{"location":"logistic_regression/#binary-logistic-regression","text":"Say we're given data on student exam results and our goal is to predict whether a student will pass or fail based on number of hours slept and hours spent studying. We have two features (hours slept, hours studied) and two classes: passed (1) and failed (0). Studied Slept Passed 4.85 9.63 1 8.62 3.23 0 5.43 8.23 1 9.21 6.34 0 Graphically we could represent our data with a scatter plot.","title":"Binary logistic regression"},{"location":"logistic_regression/#sigmoid-activation","text":"In order to map predicted values to probabilities, we use the :ref: sigmoid <activation_sigmoid> function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.","title":"Sigmoid activation"},{"location":"logistic_regression/#math","text":"\\[ S(z) = \\frac{1} {1 + e^{-z}} \\] Note \\(s(z)\\) = output between 0 and 1 (probability estimate) \\(z\\) = input to the function (your algorithm's prediction e.g. mx + b) \\(e\\) = base of natural log","title":"Math"},{"location":"logistic_regression/#graph","text":"","title":"Graph"},{"location":"logistic_regression/#code","text":"sigmoid.py import numpy as np def sigmoid ( z ): return 1.0 / ( 1 + np . exp ( - z ))","title":"Code"},{"location":"logistic_regression/#decision-boundary","text":"Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class (true/false, cat/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 2. \\[ p \\geq 0.5, class=1 \\] \\[ p < 0.5, class=0 \\] For example, if our threshold was 0.5 and our prediction function returned 0.7, we would classify this observation as positive. If our prediction was 0.2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability.","title":"Decision boundary"},{"location":"logistic_regression/#making-predictions","text":"Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function. A prediction function in logistic regression returns the probability of our observation being positive, True, or \"Yes\". We call this class 1 and its notation is \\(P(class=1)\\) . As the probability gets closer to 1, our model is more confident that the observation is in class 1.","title":"Making predictions"},{"location":"logistic_regression/#math_1","text":"Let's use the same :ref: multiple linear regression <multiple_linear_regression_predict> equation from our linear regression tutorial. \\[ z = W_0 + W_1\\cdot Studied + W_2\\cdot Slept \\] This time however we will transform the output using the sigmoid function to return a probability value between 0 and 1. \\[ P(class=1) = \\frac{1} {1 + e^{-z}} \\] If the model returns 0.4 it believes there is only a 40% chance of passing. If our decision boundary was 0.5, we would categorize this observation as \"Fail.\"\"","title":"Math"},{"location":"logistic_regression/#code_1","text":"We wrap the sigmoid function over the same prediction function we used in :ref: multiple linear regression <multiple_linear_regression_predict> predict.py import numpy from sigmoid import sigmoid def predict ( features , weights ): ''' Returns 1D array of probabilities that the class label == 1 ''' z = np . dot ( features , weights ) return sigmoid ( z )","title":"Code"},{"location":"logistic_regression/#cost-function","text":"Unfortunately we can't (or at least shouldn't) use the same cost function :ref: mse as we did for linear regression. Why? There is a great math explanation in chapter 3 of Michael Neilson's deep learning book 5 , but for now I'll simply say it's because our prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.","title":"Cost function"},{"location":"logistic_regression/#math_2","text":"Instead of Mean Squared Error, we use a cost function called :ref: loss_cross_entropy , also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for \\(y=1\\) and one for \\(y=0\\) . The benefits of taking the logarithm reveal themselves when you look at the cost function graphs for y=1 and y=0. These smooth monotonic functions 7 (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost. Image from Andrew Ng's slides on logistic regression The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions! The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function.","title":"Math"},{"location":"logistic_regression/#above-functions-compressed-into-one","text":"Multiplying by \\(y\\) and \\((1-y)\\) in the above equation is a sneaky trick that let's us use the same equation to solve for both y=1 and y=0 cases. If y=0, the first side cancels out. If y=1, the second side cancels out. In both cases we only perform the operation we need to perform.","title":"Above functions compressed into one"},{"location":"logistic_regression/#vectorized-cost-function","text":"","title":"Vectorized cost function"},{"location":"logistic_regression/#code_2","text":"cost_function.py def cost_function ( features , labels , weights ): ''' Using Mean Absolute Error Features:(100,3) Labels: (100,1) Weights:(3,1) Returns 1D matrix of predictions Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels) ''' observations = len ( labels ) predictions = predict ( features , weights ) #Take the error when label=1 class1_cost = - labels * np . log ( predictions ) #Take the error when label=0 class2_cost = ( 1 - labels ) * np . log ( 1 - predictions ) #Take the sum of both costs cost = class1_cost - class2_cost #Take the average cost cost = cost . sum () / observations return cost","title":"Code"},{"location":"logistic_regression/#gradient-descent","text":"To minimize our cost, we use :doc: gradient_descent just like before in :doc: linear_regression . There are other more sophisticated optimization algorithms out there such as conjugate gradient like :ref: optimizers_lbfgs , but you don't have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things!","title":"Gradient descent"},{"location":"logistic_regression/#math_3","text":"One of the neat properties of the sigmoid function is its derivative is easy to calculate. If you're curious, there is a good walk-through derivation on stack overflow 6 . Michael Neilson also covers the topic in chapter 3 of his book. \\[ \\begin{align} s'(z) & = s(z)(1 - s(z)) \\end{align} \\] Which leads to an equally beautiful and convenient cost function derivative: \\[ C' = x(s(z) - y) \\] Note \\(C'\\) is the derivative of cost with respect to weights \\(y\\) is the actual class label (0 or 1) \\(s(z)\\) is your model's prediction \\(x\\) is your feature or feature vector. Notice how this gradient is the same as the :ref: mse gradient, the only difference is the hypothesis function.","title":"Math"},{"location":"logistic_regression/#pseudocode","text":"Repeat { 1. Calculate gradient average 2. Multiply by learning rate 3. Subtract from weights }","title":"Pseudocode"},{"location":"logistic_regression/#code_3","text":"update_weights.py def update_weights ( features , labels , weights , lr ): ''' Vectorized Gradient Descent Features:(200, 3) Labels: (200, 1) Weights:(3, 1) ''' N = len ( features ) #1 - Get Predictions predictions = predict ( features , weights ) #2 Transpose features from (200, 3) to (3, 200) # So we can multiply w the (200,1) cost matrix. # Returns a (3,1) matrix holding 3 partial derivatives -- # one for each feature -- representing the aggregate # slope of the cost function across all observations gradient = np . dot ( features . T , predictions - labels ) #3 Take the average cost derivative for each feature gradient /= N #4 - Multiply the gradient by our learning rate gradient *= lr #5 - Subtract from our weights to minimize cost weights -= gradient return weights","title":"Code"},{"location":"logistic_regression/#mapping-probabilities-to-classes","text":"The final step is assign class labels (0 or 1) to our predicted probabilities.","title":"Mapping probabilities to classes"},{"location":"logistic_regression/#decision-boundary_1","text":"decision_boundary.py def decision_boundary ( prob ): return 1 if prob >= .5 else 0","title":"Decision boundary"},{"location":"logistic_regression/#convert-probabilities-to-classes","text":"classify.py def classify ( predictions ): ''' input - N element array of predictions between 0 and 1 output - N element array of 0s (False) and 1s (True) ''' decision_boundary = np . vectorize ( decision_boundary ) return decision_boundary ( predictions ) . flatten ()","title":"Convert probabilities to classes"},{"location":"logistic_regression/#example-output","text":"Probabilities = [ 0.967, 0.448, 0.015, 0.780, 0.978, 0.004] Classifications = [1, 0, 0, 1, 1, 0]","title":"Example output"},{"location":"logistic_regression/#training","text":"Our training code is the same as we used for :ref: linear regression <simple_linear_regression_training> . train.py def train ( features , labels , weights , lr , iters ): cost_history = [] for i in range ( iters ): weights = update_weights ( features , labels , weights , lr ) #Calculate error for auditing purposes cost = cost_function ( features , labels , weights ) cost_history . append ( cost ) # Log Progress if i % 1000 == 0 : print \"iter: \" + str ( i ) + \" cost: \" + str ( cost ) return weights , cost_history","title":"Training"},{"location":"logistic_regression/#model-evaluation","text":"If our model is working, we should see our cost decrease after every iteration. iter: 0 cost: 0.635 iter: 1000 cost: 0.302 iter: 2000 cost: 0.264 Final cost: 0.2487. Final weights: [-8.197, .921, .738]","title":"Model evaluation"},{"location":"logistic_regression/#cost-history","text":"","title":"Cost history"},{"location":"logistic_regression/#accuracy","text":":ref: Accuracy <glossary_accuracy> measures how correct our predictions were. In this case we simply compare predicted labels to true labels and divide by the total. accuracy.py def accuracy ( predicted_labels , actual_labels ): diff = predicted_labels - actual_labels return 1.0 - ( float ( np . count_nonzero ( diff )) / len ( diff ))","title":"Accuracy"},{"location":"logistic_regression/#decision-boundary_2","text":"Another helpful technique is to plot the decision boundary on top of our predictions to see how our labels compare to the actual labels. This involves plotting our predicted probabilities and coloring them with their true labels.","title":"Decision boundary"},{"location":"logistic_regression/#code-to-plot-the-decision-boundary","text":"plot_decision_boundary.py def plot_decision_boundary ( trues , falses ): fig = plt . figure () ax = fig . add_subplot ( 111 ) no_of_preds = len ( trues ) + len ( falses ) ax . scatter ([ i for i in range ( len ( trues ))], trues , s = 25 , c = 'b' , marker = \"o\" , label = 'Trues' ) ax . scatter ([ i for i in range ( len ( falses ))], falses , s = 25 , c = 'r' , marker = \"s\" , label = 'Falses' ) plt . legend ( loc = 'upper right' ); ax . set_title ( \"Decision Boundary\" ) ax . set_xlabel ( 'N/2' ) ax . set_ylabel ( 'Predicted Probability' ) plt . axhline ( .5 , color = 'black' ) plt . show ()","title":"Code to plot the decision boundary"},{"location":"logistic_regression/#multiclass-logistic-regression","text":"Instead of \\(y = {0,1}\\) we will expand our definition so that \\(y = {0,1...n}\\) . Basically we re-run binary classification multiple times, once for each class.","title":"Multiclass logistic regression"},{"location":"logistic_regression/#procedure","text":"Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?). For each class... Predict the probability the observations are in that single class. prediction = \\(max(\\text{probability of the classes})\\) For each sub-problem, we select one class (YES) and lump all the others into a second class (NO). Then we take the class with the highest predicted value.","title":"Procedure"},{"location":"logistic_regression/#softmax-activation","text":"The softmax function (softargmax or normalized exponential function) is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval [ 0 , 1 ] , and the components will add up to 1, so that they can be interpreted as probabilities. The standard (unit) softmax function is defined by the formula \\[ \\begin{align} \u03c3(z_i) = \\frac{e^{z_{(i)}}}{\\sum_{j=1}^K e^{z_{(j)}}}\\ \\ \\ for\\ i=1,.,.,.,K\\ and\\ z=z_1,.,.,.,z_K \\end{align} \\] In words: we apply the standard exponential function to each element \\(z_i\\) of the input vector \\(z\\) and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector \\(\u03c3(z)\\) is 1. 9","title":"Softmax activation"},{"location":"logistic_regression/#scikit-learn-example","text":"Let's compare our performance to the LogisticRegression model provided by scikit-learn 8 . .browserslistrc import sklearn from sklearn.linear_model import LogisticRegression from sklearn.cross_validation import train_test_split # Normalize grades to values between 0 and 1 for more efficient computation normalized_range = sklearn . preprocessing . MinMaxScaler ( feature_range = ( - 1 , 1 )) # Extract Features + Labels labels . shape = ( 100 ,) #scikit expects this features = normalized_range . fit_transform ( features ) # Create Test/Train features_train , features_test , labels_train , labels_test = train_test_split ( features , labels , test_size = 0.4 ) # Scikit Logistic Regression scikit_log_reg = LogisticRegression () scikit_log_reg . fit ( features_train , labels_train ) #Score is Mean Accuracy scikit_score = clf . score ( features_test , labels_test ) print ( 'Scikit score: ' , scikit_score ) #Our Mean Accuracy observations , features , labels , weights = run () probabilities = predict ( features , weights ) . flatten () classifications = classifier ( probabilities ) our_acc = accuracy ( classifications , labels . flatten ()) print ( 'Our score: ' , our_acc ) Scikit score: 0.88. Our score: 0.89","title":"Scikit-Learn example"},{"location":"logistic_regression/#references","text":"http://www.holehouse.org/mlclass/06_Logistic_Regression.html \u21a9 http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning \u21a9 https://scilab.io/machine-learning-logistic-regression-tutorial/ \u21a9 https://github.com/perborgen/LogisticRegression/blob/master/logistic.py \u21a9 http://neuralnetworksanddeeplearning.com/chap3.html \u21a9 http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x \u21a9 https://en.wikipedia.org/wiki/Monotoniconotonic_function \u21a9 http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression> \u21a9 https://en.wikipedia.org/wiki/Softmax_function \u21a9","title":"References"}]}